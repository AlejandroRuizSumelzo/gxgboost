#include dynargs.dec
#include gxgboost.sdf

proc (1) = xgbCreateLearningParams();
    struct xgbLearningParams params;
    params.num_round = 10;
    params.objective = "reg:linear";
    params.base_score = 0.5;
    params.eval_metric = "";
    params.seed = 0;
    params.tweedie_variance_power = 1.5;
    params.verbose = 0;
    retp(params);
endp;

proc (1) = xgbCreateTreeParams();
    struct xgbTreeParams params;
    params.eta = 0.3;
    params.gamma = 0;
    params.max_depth = 6;
    params.min_child_weight = 1;
    params.subsample = 1;
    params.colsample_bytree = 1;
    params.colsample_bylevel = 1;
    params.lambda = 1;
    params.alpha = 0;
    params.tree_method = "auto";
    params.sketch_eps = 0.03;
    params.scale_pos_weight = 1;
    params.updater = ""; //"grow_colmaker,prune";
    params.refresh_leaf = 1;
    params.process_type = "default";
    params.grow_policy = "depthwise";
    params.max_leaves = 0;
    params.max_bin = 256;
    params.predictor = "cpu_predictor";
    retp(params);
endp;

proc (1) = xgbCreateDartParams();
    struct xgbDartParams params;
    params.tree = xgbCreateTreeParams();
    params.sample_type = "uniform";
    params.normalize_type = "tree";
    params.rate_drop = 0.0;
    params.one_drop = 0;
    params.skip_drop = 0.0;
    retp(params);
endp;

proc (1) = xgbCreateLinearParams();
    struct xgbLinearParams params;
    params.lambda = 0;
    params.alpha = 0;
    params.updater = "shotgun";
    retp(params);
endp;

proc (1) = xgbCreateCtl(booster);
    if booster $== "tree";
        struct xgbTree ctl_tree;
        ctl_tree.learning = xgbCreateLearningParams();
        ctl_tree.params = xgbCreateTreeParams();
        retp(ctl_tree);
    elseif booster $== "dart";
        struct xgbDart ctl_dart;
        ctl_dart.learning = xgbCreateLearningParams();
        ctl_dart.params = xgbCreateDartParams();
        retp(ctl_dart);
    elseif booster $== "linear";
        struct xgbLinear ctl_linear;
        ctl_linear.learning = xgbCreateLearningParams();
        ctl_linear.params = xgbCreateLinearParams();
        retp(ctl_linear);
    endif;
    
    errorlog("Invalid booster type. Valid options are 'tree', 'dart', and 'linear'");
    end;
    
    retp(0);
endp;

proc (1) = xgbCreatePredictParams();
    struct xgbPredictParams params;
    params.output_margin = 0;
    params.ntree_limit = 0;
    params.pred_leaf = 0;
    params.pred_contribs = 0;
    params.approx_contribs = 0;
    params.pred_interactions = 0;
    retp(params);
endp;

proc (20) = _xgb_split_tree_params(struct xgbTreeParams params);
    retp(params.eta, params.gamma, params.max_depth, 
    params.min_child_weight, params.max_delta_step, 
    params.subsample, params.colsample_bytree, 
    params.colsample_bylevel, params.lambda, params.alpha,
    params.tree_method, params.sketch_eps, params.scale_pos_weight, 
    params.updater, params.refresh_leaf, params.process_type, 
    params.grow_policy, params.max_leaves, params.max_bin, 
    params.predictor);
endp;

proc (7) = _xgb_split_learning_params(struct xgbLearningParams params);
    retp(params.num_round, params.objective, params.base_score, 
    params.eval_metric, params.seed, params.tweedie_variance_power,
    params.verbose);
endp;

proc (1) = _xgb_train_tree(y, x, struct xgbTree *ctl);
    local inputTrainRows, inputTrainCols;
    inputTrainRows = rows(x);
    inputTrainCols = cols(x);
    
    local dloModelPtr, dloModelRows, retModel;
    clear dloModelPtr, dloModelRows;
    
    local num_round, objective, base_score, eval_metric, seed, tweedie_variance_power, verbose;
    { num_round, objective, base_score, eval_metric, seed, 
    tweedie_variance_power, verbose } = _xgb_split_learning_params(ctl->learning);
    
    local eta, gamma_, max_depth, min_child_weight, max_delta_step, subsample,
    colsample_bytree, colsample_bylevel, lambda, alpha, tree_method, sketch_eps,
    scale_pos_weight, updater, refresh_leaf, process_type, grow_policy, max_leaves,
    max_bin, predictor;
    
    { eta, gamma_, max_depth, min_child_weight, max_delta_step, subsample,
    colsample_bytree, colsample_bylevel, lambda, alpha, tree_method, sketch_eps,
    scale_pos_weight, updater, refresh_leaf, process_type, grow_policy, max_leaves,
    max_bin, predictor } = _xgb_split_tree_params(ctl->params);
    
    dllcall -v gxgboost_train_tree(
        // struct InputData
        x, inputTrainRows, inputTrainCols, y,
        // struct VectorData (output)
        dloModelPtr, dloModelRows,
        // learning params
        num_round, objective, base_score, eval_metric, seed, tweedie_variance_power, verbose,
        // tree params
        eta, gamma_, max_depth, min_child_weight, max_delta_step, subsample,
            colsample_bytree, colsample_bylevel, lambda, alpha, tree_method, 
            sketch_eps, scale_pos_weight, updater, refresh_leaf, process_type, 
            grow_policy, max_leaves, max_bin, predictor
    );
    
    retModel = {};
    if dloModelRows;
        retModel = commandeerm( dloModelRows, 1, dloModelPtr );
    endif;
    retp(retModel);
endp;

proc (1) = _xgb_train_dart(y, x, struct xgbDart *ctl);
    local inputTrainRows, inputTrainCols;
    inputTrainRows = rows(x);
    inputTrainCols = cols(x);
    
    local dloModelPtr, dloModelRows, retModel;
    clear dloModelPtr, dloModelRows;
    
    local num_round, objective, base_score, eval_metric, seed, tweedie_variance_power, verbose;
    { num_round, objective, base_score, eval_metric, seed, 
    tweedie_variance_power, verbose } = _xgb_split_learning_params(ctl->learning);
    
    local eta, gamma_, max_depth, min_child_weight, max_delta_step, subsample,
    colsample_bytree, colsample_bylevel, lambda, alpha, tree_method, sketch_eps,
    scale_pos_weight, updater, refresh_leaf, process_type, grow_policy, max_leaves,
    max_bin, predictor;
    
    { eta, gamma_, max_depth, min_child_weight, max_delta_step, subsample,
    colsample_bytree, colsample_bylevel, lambda, alpha, tree_method, sketch_eps,
    scale_pos_weight, updater, refresh_leaf, process_type, grow_policy, max_leaves,
    max_bin, predictor } = _xgb_split_tree_params(ctl->params);
    
    local sample_type, normalize_type, rate_drop, one_drop, skip_drop;
    sample_type = ctl->params.sample_type;
    normalize_type = ctl->params.normalize_type;
    rate_drop = ctl->params.rate_drop;
    one_drop = ctl->params.one_drop;
    skip_drop = ctl->params.skip_drop;
    
    dllcall -v gxgboost_train_dart(
        // struct InputData
        x, inputTrainRows, inputTrainCols, y,
        // struct VectorData (output)
        dloModelPtr, dloModelRows,
        // learning params
        num_round, objective, base_score, eval_metric, seed, tweedie_variance_power, verbose,
        // tree params
        eta, gamma_, max_depth, min_child_weight, max_delta_step, subsample,
            colsample_bytree, colsample_bylevel, lambda, alpha, tree_method, 
            sketch_eps, scale_pos_weight, updater, refresh_leaf, process_type, 
            grow_policy, max_leaves, max_bin, predictor,
        // dart params
        sample_type, normalize_type, rate_drop, one_drop, skip_drop
    );
    
    retModel = {};
    if dloModelRows;
        retModel = commandeerm( dloModelRows, 1, dloModelPtr );
    endif;
    retp(retModel);
endp;

proc (1) = _xgb_train_linear(y, x, struct xgbLinear *ctl);
    local inputTrainRows, inputTrainCols;
    inputTrainRows = rows(x);
    inputTrainCols = cols(x);
    
    local dloModelPtr, dloModelRows, retModel;
    clear dloModelPtr, dloModelRows;
    
    local num_round, objective, base_score, eval_metric, seed, tweedie_variance_power, verbose;
    { num_round, objective, base_score, eval_metric, seed, 
    tweedie_variance_power, verbose } = _xgb_split_learning_params(ctl->learning);
    
    local lambda, alpha, updater;
    lambda = ctl->params.lambda;
    alpha = ctl->params.alpha;
    updater = ctl->params.updater;
    
    dllcall -v gxgboost_train_tree(
        // struct InputData
        x, inputTrainRows, inputTrainCols, y,
        // struct VectorData (output)
        dloModelPtr, dloModelRows,
        // learning params
        num_round, objective, base_score, eval_metric, seed, tweedie_variance_power, verbose,
        // linear params
        lambda, alpha, updater
    );
    
    retModel = {};
    if dloModelRows;
        retModel = commandeerm( dloModelRows, 1, dloModelPtr );
    endif;
    retp(retModel);
endp;

proc (0) = _xgbFitValidate(labels, data);
        if rows(data) != rows(labels);
        errorlog "Rows of labels ("$+ntos(rows(labels))$+") and data ("$+ntos(rows(data))$+") must match";
        end;
    elseif cols(labels) > 1;
        errorlog "labels must be a row vector";
        end;
    endif;
endp;

proc (1) = _xgbCreateCtlSmart(booster, ...);
    local nargs;
    nargs = COUNT_DYNARGS;
    
    if nargs > 1;
        errorlog "Invalid number of arguments.";
        end;
    elseif nargs == 1;
        local dynarg_type;
        dynarg_type = GET_DYNARG_TYPES;
        
        if dynarg_type != 17;
            errorlog "Params argument must be structure";
            end;
        endif;
        
        retp(sysstate(GET_ONE_DYNARG, 1));
    endif;
    
    retp(xgbCreateCtl(booster));
endp;

proc (1) = xgbTreeFit(labels, data, ...);
    dlibrary -a gxgboost;

    _xgbFitValidate(labels, data);
    
    struct xgbTree ctl;
    ctl = _xgbCreateCtlSmart("tree", ...);
    
    struct xgbModel ret;
    ret.model = _xgb_train_tree(labels, data, &ctl);
    retp(ret);
endp;

proc (1) = xgbDartFit(labels, data, ...);
    dlibrary -a gxgboost;

    _xgbFitValidate(labels, data);
    
    struct xgbDart ctl;
    ctl = _xgbCreateCtlSmart("dart", ...);
    
    struct xgbModel ret;
    ret.model = _xgb_train_dart(labels, data, &ctl);
    retp(ret);
endp;

proc (1) = xgbLinearFit(labels, data, ...);
    dlibrary -a gxgboost;

    _xgbFitValidate(labels, data);
    
    struct xgbLinear ctl;
    ctl = _xgbCreateCtlSmart("linear", ...);
    
    struct xgbModel ret;
    ret.model = _xgb_train_linear(labels, data, &ctl);
    retp(ret);
endp;

proc (1) = xgbPredict(struct xgbModel model, data, ...);
    dlibrary -a gxgboost;
    local test_data_rows, test_data_cols;
    test_data_rows = rows(data);
    test_data_cols = cols(data);
    
    local model_data, model_size;
    model_data = model.model;
    model_size = rows(model_data);

    if not model_size;
        errorlog "Please train a model first.";
        end;
    endif;
    
    local dloResultsPtr, dloResultsRows, retResults;
    clear dloResultsPtr, dloResultsRows;
    
    local nargs;
    struct xgbPredictParams params;
    
    nargs = COUNT_DYNARGS;
    
    if nargs > 1;
        errorlog "Invalid number of arguments.";
        end;
    elseif nargs == 1;
        params = sysstate(GET_ONE_DYNARG, 1);
    else;
        params = xgbCreatePredictParams();
    endif;
    
    local output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions;
    output_margin = params.output_margin;
    ntree_limit = params.ntree_limit;
    pred_leaf = params.pred_leaf;
    pred_contribs = params.pred_contribs;
    approx_contribs = params.approx_contribs;
    pred_interactions = params.pred_interactions;
    
    dllcall -v gxgboost_predict(
    // struct InputData
    data, test_data_rows, test_data_cols,
    // struct VectorData (input model)
    model_data, model_size,
    // struct VectorData (output results)
    dloResultsPtr, dloResultsRows,
    // predict params
    output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions
    );
    
    retResults = {};
    if dloResultsRows;
        retResults = commandeerm( dloResultsRows, 1, dloResultsPtr );
    endif;
    
    retp(retResults);
endp;

#IFMINKERNELVERSION(18)
#else
proc (1) = __FILE_DIR();
    retp(getGAUSSHome() $+ "examples/");
endp;
#endif

